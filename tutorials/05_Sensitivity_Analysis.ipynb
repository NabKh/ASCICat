{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Sensitivity Analysis\n",
    "\n",
    "## Understanding How Weights Affect Catalyst Rankings\n",
    "\n",
    "This tutorial covers comprehensive sensitivity analysis for ASCI scoring:\n",
    "\n",
    "1. Weight sweep analysis\n",
    "2. Ternary diagrams showing optimal regions\n",
    "3. Robustness metrics for catalyst selection\n",
    "4. Identifying weight-insensitive top performers\n",
    "5. Statistical correlation analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from ascicat import ASCICalculator\n",
    "from ascicat.sensitivity import SensitivityAnalyzer\n",
    "from ascicat.config import REACTION_CONFIGS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.tri as tri\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 100,\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 13\n",
    "})\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HER data\n",
    "calc = ASCICalculator(reaction='HER', scoring_method='linear')\n",
    "calc.load_data('../data/HER_clean.csv')\n",
    "\n",
    "# Calculate baseline results (equal weights)\n",
    "baseline_results = calc.calculate_asci(w_a=0.33, w_s=0.33, w_c=0.34)\n",
    "\n",
    "print(f\"Loaded {len(baseline_results)} catalysts\")\n",
    "print(f\"Baseline ASCI (equal weights): mean={baseline_results['ASCI'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Weight Grid (Simplex Sampling)\n",
    "\n",
    "Create a grid of weight combinations that sum to 1, covering the entire weight space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_grid(n_points=21, min_weight=0.1):\n",
    "    \"\"\"\n",
    "    Generate weight combinations on the simplex.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_points : int, number of points along each dimension\n",
    "    min_weight : float, minimum weight for each criterion\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of (w_a, w_s, w_c) tuples\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    step = (1 - 3 * min_weight) / (n_points - 1) if n_points > 1 else 0\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        w_a = min_weight + i * step\n",
    "        for j in range(n_points - i):\n",
    "            w_s = min_weight + j * step\n",
    "            w_c = 1 - w_a - w_s\n",
    "            if w_c >= min_weight - 1e-9:  # Allow small numerical tolerance\n",
    "                weights.append((round(w_a, 4), round(w_s, 4), round(w_c, 4)))\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Generate weight grid\n",
    "weight_grid = generate_weight_grid(n_points=21, min_weight=0.1)\n",
    "print(f\"Generated {len(weight_grid)} weight combinations\")\n",
    "print(f\"\\nSample weights:\")\n",
    "for w in weight_grid[:5]:\n",
    "    print(f\"  Activity={w[0]:.2f}, Stability={w[1]:.2f}, Cost={w[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Sweep Analysis\n",
    "\n",
    "Calculate ASCI scores for all weight combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run weight sweep\n",
    "sweep_results = []\n",
    "\n",
    "print(\"Running weight sweep...\")\n",
    "for idx, (w_a, w_s, w_c) in enumerate(weight_grid):\n",
    "    # Create fresh calculator for each weight combination\n",
    "    temp_calc = ASCICalculator(reaction='HER', scoring_method='linear')\n",
    "    temp_calc.load_data('../data/HER_clean.csv')\n",
    "    results = temp_calc.calculate_asci(w_a=w_a, w_s=w_s, w_c=w_c)\n",
    "    \n",
    "    # Get top catalyst\n",
    "    top_1 = results.nsmallest(1, 'rank')\n",
    "    \n",
    "    sweep_results.append({\n",
    "        'w_a': w_a,\n",
    "        'w_s': w_s,\n",
    "        'w_c': w_c,\n",
    "        'top_catalyst': top_1['symbol'].values[0] if 'symbol' in top_1.columns else top_1.index[0],\n",
    "        'top_ASCI': top_1['ASCI'].values[0],\n",
    "        'mean_ASCI': results['ASCI'].mean(),\n",
    "        'std_ASCI': results['ASCI'].std()\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"  Processed {idx + 1}/{len(weight_grid)} combinations...\")\n",
    "\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "print(f\"\\nWeight sweep complete: {len(sweep_df)} combinations analyzed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top catalyst frequency\n",
    "catalyst_frequency = sweep_df['top_catalyst'].value_counts()\n",
    "\n",
    "print(\"Top Catalyst Frequency Across Weight Space\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Catalyst':<20} {'Frequency':<12} {'Percentage'}\")\n",
    "print(\"-\"*50)\n",
    "for cat, freq in catalyst_frequency.head(10).items():\n",
    "    pct = 100 * freq / len(sweep_df)\n",
    "    print(f\"{cat:<20} {freq:<12} {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ternary Diagram: Weight Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ternary(w_a, w_s, w_c, values, title, cmap='viridis', label='Value'):\n",
    "    \"\"\"\n",
    "    Create a ternary diagram for visualizing weight space.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    w_a, w_s, w_c : arrays of weights\n",
    "    values : array of values to color by\n",
    "    title : str, plot title\n",
    "    cmap : str, colormap name\n",
    "    label : str, colorbar label\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "    \n",
    "    # Convert to cartesian coordinates for ternary plot\n",
    "    # Using: x = w_s + 0.5*w_c, y = sqrt(3)/2 * w_c\n",
    "    w_a = np.array(w_a)\n",
    "    w_s = np.array(w_s)\n",
    "    w_c = np.array(w_c)\n",
    "    \n",
    "    x = w_s + 0.5 * w_c\n",
    "    y = np.sqrt(3) / 2 * w_c\n",
    "    \n",
    "    # Create triangulation\n",
    "    triang = tri.Triangulation(x, y)\n",
    "    \n",
    "    # Plot contour\n",
    "    tcf = ax.tricontourf(triang, values, levels=20, cmap=cmap)\n",
    "    ax.tricontour(triang, values, levels=10, colors='white', linewidths=0.5, alpha=0.5)\n",
    "    \n",
    "    # Add scatter points\n",
    "    ax.scatter(x, y, c=values, cmap=cmap, s=20, edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    # Draw triangle outline\n",
    "    triangle = plt.Polygon([[0, 0], [1, 0], [0.5, np.sqrt(3)/2]], \n",
    "                           fill=False, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(triangle)\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(0, -0.05, 'Activity', ha='center', va='top', fontsize=12, fontweight='bold')\n",
    "    ax.text(1, -0.05, 'Stability', ha='center', va='top', fontsize=12, fontweight='bold')\n",
    "    ax.text(0.5, np.sqrt(3)/2 + 0.05, 'Cost', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(tcf, ax=ax, shrink=0.7)\n",
    "    cbar.set_label(label, fontsize=11)\n",
    "    \n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.15, 1.0)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Plot ternary diagram of top ASCI scores\n",
    "fig, ax = plot_ternary(\n",
    "    sweep_df['w_a'], \n",
    "    sweep_df['w_s'], \n",
    "    sweep_df['w_c'],\n",
    "    sweep_df['top_ASCI'],\n",
    "    'Best ASCI Score Across Weight Space',\n",
    "    cmap='viridis',\n",
    "    label='Top ASCI Score'\n",
    ")\n",
    "\n",
    "output_dir = '../results/tutorial_sensitivity'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plt.savefig(f'{output_dir}/ternary_top_ASCI.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Robustness Analysis: Top-N Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate top-10 frequency for each catalyst\n",
    "top_n = 10\n",
    "top_n_counts = defaultdict(int)\n",
    "\n",
    "for idx, (w_a, w_s, w_c) in enumerate(weight_grid):\n",
    "    temp_calc = ASCICalculator(reaction='HER', scoring_method='linear')\n",
    "    temp_calc.load_data('../data/HER_clean.csv')\n",
    "    results = temp_calc.calculate_asci(w_a=w_a, w_s=w_s, w_c=w_c)\n",
    "    \n",
    "    top_catalysts = results.nsmallest(top_n, 'rank')\n",
    "    \n",
    "    if 'symbol' in top_catalysts.columns:\n",
    "        for cat in top_catalysts['symbol']:\n",
    "            top_n_counts[cat] += 1\n",
    "    else:\n",
    "        for cat in top_catalysts.index:\n",
    "            top_n_counts[cat] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "robustness_df = pd.DataFrame([\n",
    "    {'catalyst': cat, 'top10_count': count, 'top10_frequency': count / len(weight_grid)}\n",
    "    for cat, count in top_n_counts.items()\n",
    "]).sort_values('top10_count', ascending=False)\n",
    "\n",
    "print(f\"Robustness Analysis: Top-10 Frequency\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Catalysts appearing in top-10 across {len(weight_grid)} weight combinations:\\n\")\n",
    "print(robustness_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize robustness\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_20_robust = robustness_df.head(20)\n",
    "colors = plt.cm.viridis(top_20_robust['top10_frequency'] / top_20_robust['top10_frequency'].max())\n",
    "\n",
    "bars = ax.barh(range(len(top_20_robust)), top_20_robust['top10_frequency'], color=colors)\n",
    "ax.set_yticks(range(len(top_20_robust)))\n",
    "ax.set_yticklabels(top_20_robust['catalyst'])\n",
    "ax.set_xlabel('Frequency in Top-10', fontsize=12)\n",
    "ax.set_title('Most Robust Catalysts: Top-10 Frequency Across Weight Space', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add frequency labels\n",
    "for i, (_, row) in enumerate(top_20_robust.iterrows()):\n",
    "    ax.text(row['top10_frequency'] + 0.01, i, f\"{row['top10_frequency']:.2f}\", \n",
    "            va='center', fontsize=9)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/robustness_top10.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rank Correlation Analysis\n",
    "\n",
    "How consistent are rankings across different weight configurations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select representative weight configurations\n",
    "representative_weights = [\n",
    "    (0.33, 0.33, 0.34, 'Equal'),\n",
    "    (0.6, 0.2, 0.2, 'Activity-focused'),\n",
    "    (0.2, 0.6, 0.2, 'Stability-focused'),\n",
    "    (0.2, 0.2, 0.6, 'Cost-focused'),\n",
    "    (0.5, 0.3, 0.2, 'Activity-Stability'),\n",
    "    (0.3, 0.2, 0.5, 'Activity-Cost')\n",
    "]\n",
    "\n",
    "# Calculate rankings for each configuration\n",
    "rankings = {}\n",
    "\n",
    "for w_a, w_s, w_c, name in representative_weights:\n",
    "    temp_calc = ASCICalculator(reaction='HER', scoring_method='linear')\n",
    "    temp_calc.load_data('../data/HER_clean.csv')\n",
    "    results = temp_calc.calculate_asci(w_a=w_a, w_s=w_s, w_c=w_c)\n",
    "    rankings[name] = results['rank'].values\n",
    "\n",
    "# Calculate Kendall's Tau correlation matrix\n",
    "config_names = [name for _, _, _, name in representative_weights]\n",
    "n_configs = len(config_names)\n",
    "tau_matrix = np.zeros((n_configs, n_configs))\n",
    "\n",
    "for i, name1 in enumerate(config_names):\n",
    "    for j, name2 in enumerate(config_names):\n",
    "        tau, _ = stats.kendalltau(rankings[name1], rankings[name2])\n",
    "        tau_matrix[i, j] = tau\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(tau_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(n_configs))\n",
    "ax.set_yticks(range(n_configs))\n",
    "ax.set_xticklabels(config_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(config_names)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(n_configs):\n",
    "    for j in range(n_configs):\n",
    "        text = ax.text(j, i, f\"{tau_matrix[i, j]:.2f}\",\n",
    "                       ha='center', va='center', fontsize=10,\n",
    "                       color='white' if tau_matrix[i, j] < 0.5 else 'black')\n",
    "\n",
    "ax.set_title(\"Kendall's Tau Rank Correlation Between Weight Configurations\",\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label(\"Kendall's Tau\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/kendall_tau_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ASCI Score Trajectories\n",
    "\n",
    "Track how individual catalyst scores change across weight space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 catalysts from equal weights\n",
    "top_10_baseline = baseline_results.nsmallest(10, 'rank')\n",
    "if 'symbol' in top_10_baseline.columns:\n",
    "    tracked_catalysts = top_10_baseline['symbol'].tolist()\n",
    "else:\n",
    "    tracked_catalysts = top_10_baseline.index.tolist()\n",
    "\n",
    "# Create activity weight sweep (keeping stability=cost)\n",
    "activity_weights = np.linspace(0.1, 0.8, 15)\n",
    "\n",
    "trajectory_data = {cat: {'w_a': [], 'ASCI': [], 'rank': []} for cat in tracked_catalysts}\n",
    "\n",
    "for w_a in activity_weights:\n",
    "    w_s = (1 - w_a) / 2\n",
    "    w_c = (1 - w_a) / 2\n",
    "    \n",
    "    temp_calc = ASCICalculator(reaction='HER', scoring_method='linear')\n",
    "    temp_calc.load_data('../data/HER_clean.csv')\n",
    "    results = temp_calc.calculate_asci(w_a=w_a, w_s=w_s, w_c=w_c)\n",
    "    \n",
    "    for cat in tracked_catalysts:\n",
    "        if 'symbol' in results.columns:\n",
    "            match = results[results['symbol'] == cat]\n",
    "        else:\n",
    "            match = results.loc[[cat]] if cat in results.index else pd.DataFrame()\n",
    "        \n",
    "        if not match.empty:\n",
    "            trajectory_data[cat]['w_a'].append(w_a)\n",
    "            trajectory_data[cat]['ASCI'].append(match['ASCI'].values[0])\n",
    "            trajectory_data[cat]['rank'].append(match['rank'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ASCI trajectories\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(tracked_catalysts)))\n",
    "\n",
    "# Panel A: ASCI Score trajectories\n",
    "for i, cat in enumerate(tracked_catalysts):\n",
    "    data = trajectory_data[cat]\n",
    "    axes[0].plot(data['w_a'], data['ASCI'], 'o-', color=colors[i], \n",
    "                 linewidth=2, markersize=5, label=cat)\n",
    "\n",
    "axes[0].set_xlabel('Activity Weight', fontsize=12)\n",
    "axes[0].set_ylabel('ASCI Score', fontsize=12)\n",
    "axes[0].set_title('A. ASCI Score vs Activity Weight', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\n",
    "axes[0].axvline(0.33, color='gray', linestyle='--', alpha=0.5, label='Equal weights')\n",
    "\n",
    "# Panel B: Rank trajectories\n",
    "for i, cat in enumerate(tracked_catalysts):\n",
    "    data = trajectory_data[cat]\n",
    "    axes[1].plot(data['w_a'], data['rank'], 'o-', color=colors[i], \n",
    "                 linewidth=2, markersize=5, label=cat)\n",
    "\n",
    "axes[1].set_xlabel('Activity Weight', fontsize=12)\n",
    "axes[1].set_ylabel('Rank', fontsize=12)\n",
    "axes[1].set_title('B. Rank vs Activity Weight', fontsize=13, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_ylim(50, 0)\n",
    "axes[1].axvline(0.33, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/score_trajectories.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Weight Region Analysis\n",
    "\n",
    "Which catalyst is #1 in different regions of weight space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize weight space into regions\n",
    "def categorize_weight(w_a, w_s, w_c):\n",
    "    \"\"\"Categorize weight combination into a named region.\"\"\"\n",
    "    dominant = max(w_a, w_s, w_c)\n",
    "    if dominant == w_a and w_a > 0.4:\n",
    "        return 'Activity-dominant'\n",
    "    elif dominant == w_s and w_s > 0.4:\n",
    "        return 'Stability-dominant'\n",
    "    elif dominant == w_c and w_c > 0.4:\n",
    "        return 'Cost-dominant'\n",
    "    else:\n",
    "        return 'Balanced'\n",
    "\n",
    "sweep_df['region'] = sweep_df.apply(\n",
    "    lambda row: categorize_weight(row['w_a'], row['w_s'], row['w_c']), axis=1\n",
    ")\n",
    "\n",
    "# Analyze top catalyst by region\n",
    "print(\"Top Catalyst by Weight Region\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for region in ['Activity-dominant', 'Stability-dominant', 'Cost-dominant', 'Balanced']:\n",
    "    region_data = sweep_df[sweep_df['region'] == region]\n",
    "    if len(region_data) > 0:\n",
    "        top_in_region = region_data['top_catalyst'].value_counts().head(3)\n",
    "        print(f\"\\n{region} ({len(region_data)} weight combinations):\")\n",
    "        for cat, count in top_in_region.items():\n",
    "            pct = 100 * count / len(region_data)\n",
    "            print(f\"  {cat}: {count} times ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize region dominance\n",
    "region_counts = sweep_df.groupby(['region', 'top_catalyst']).size().unstack(fill_value=0)\n",
    "\n",
    "# Get top 5 catalysts overall\n",
    "top_5_overall = catalyst_frequency.head(5).index.tolist()\n",
    "region_counts_filtered = region_counts[top_5_overall]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "region_counts_filtered.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_xlabel('Weight Region', fontsize=12)\n",
    "ax.set_ylabel('Count (as #1 catalyst)', fontsize=12)\n",
    "ax.set_title('Top Catalyst Dominance by Weight Region', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Catalyst', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{output_dir}/region_dominance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Sensitivity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all sensitivity analysis results\n",
    "\n",
    "# Weight sweep results\n",
    "sweep_df.to_csv(f'{output_dir}/weight_sweep_results.csv', index=False)\n",
    "\n",
    "# Robustness ranking\n",
    "robustness_df.to_csv(f'{output_dir}/robustness_ranking.csv', index=False)\n",
    "\n",
    "# Kendall's Tau matrix\n",
    "tau_df = pd.DataFrame(tau_matrix, index=config_names, columns=config_names)\n",
    "tau_df.to_csv(f'{output_dir}/kendall_tau_matrix.csv')\n",
    "\n",
    "print(f\"Sensitivity analysis results saved to: {output_dir}\")\n",
    "print(\"\\nFiles created:\")\n",
    "for f in sorted(os.listdir(output_dir)):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sensitivity Analysis Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Most robust catalyst\n",
    "most_robust = robustness_df.iloc[0]\n",
    "print(f\"\\n1. Most Robust Catalyst: {most_robust['catalyst']}\")\n",
    "print(f\"   - In top-10 across {most_robust['top10_frequency']*100:.1f}% of weight combinations\")\n",
    "\n",
    "# Most frequent #1\n",
    "most_frequent_1 = catalyst_frequency.head(1)\n",
    "print(f\"\\n2. Most Frequent #1 Catalyst: {most_frequent_1.index[0]}\")\n",
    "print(f\"   - Ranked #1 in {100*most_frequent_1.values[0]/len(sweep_df):.1f}% of configurations\")\n",
    "\n",
    "# Average rank correlation\n",
    "avg_tau = tau_matrix[np.triu_indices(n_configs, k=1)].mean()\n",
    "print(f\"\\n3. Average Rank Correlation (Kendall's Tau): {avg_tau:.3f}\")\n",
    "print(f\"   - Rankings are {'highly' if avg_tau > 0.8 else 'moderately' if avg_tau > 0.6 else 'weakly'} consistent across configurations\")\n",
    "\n",
    "# Weight sensitivity\n",
    "asci_range = sweep_df['top_ASCI'].max() - sweep_df['top_ASCI'].min()\n",
    "print(f\"\\n4. Top ASCI Score Range: {sweep_df['top_ASCI'].min():.3f} - {sweep_df['top_ASCI'].max():.3f}\")\n",
    "print(f\"   - Variation: {asci_range:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Weight Selection\n",
    "\n",
    "Based on this sensitivity analysis:\n",
    "\n",
    "1. **For robust results**: Choose catalysts that appear frequently in the top-10 across all weight configurations\n",
    "2. **For application-specific**: Use domain knowledge to emphasize the most critical criterion\n",
    "3. **For uncertainty**: Report rankings for multiple weight configurations (at minimum: equal, activity-focused, cost-focused)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the ASCICat tutorial series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
